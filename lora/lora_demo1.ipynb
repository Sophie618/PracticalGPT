{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89767af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6b3049",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, r=4, lora_alpha=1):\n",
    "        super().__init__()\n",
    "        # 1. 原来的全连接层 (模拟冻结的预训练权重)\n",
    "        self.pretrained = nn.Linear(in_features, out_features, bias=False)#提取预训练的线性层\n",
    "\n",
    "        # --- 核心：冻结它！不许更新梯度 ---\n",
    "        self.pretrained.weight.requires_grad = False \n",
    "\n",
    "        self.lora_A = nn.Parameter(torch.randn(r, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, r))\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.lora_A,a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "\n",
    "        self.scaling=lora_alpha/r#缩放因子，使无论r值变化如何LoRA的初始权重变化幅度一致\n",
    "\n",
    "    def forward(self,x):\n",
    "        #最终结果=原始结果+增量\n",
    "        #1.原始结果@W.T\n",
    "        result_base=self.pretrained(x)\n",
    "\n",
    "        #2.增量x@A.T@B.T*scaling\n",
    "        lora_process=(x@self.lora_A.T)@self.lora_B.T*self.scaling\n",
    "\n",
    "        return result_base+lora_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ff4e775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始权重参数量：1048576\n",
      "LoRA (A+B) 参数量: 16384\n",
      "节省了98.44%的显存\n",
      "输出维度： torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "#test code\n",
    "#假设拥有一个巨大的层输入1024输出1024\n",
    "layer=LoRALinear(1024,1024,r=8)\n",
    "\n",
    "total_params=sum(p.numel() for p in layer.parameters())\n",
    "trainable_params=sum(p.numel() for p in layer.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"原始权重参数量：{1024*1024}\")\n",
    "print(f\"LoRA (A+B) 参数量: {trainable_params}\")\n",
    "print(f\"节省了{(1-trainable_params/(1024*1024))*100:.2f}%的显存\")\n",
    "\n",
    "#测试前向传播\n",
    "input_data=torch.randn(1,1024)#randn是一个造数机器，生成符合标准正态分布（均值为0，方差为1）的随机数。\n",
    "out_put=layer(input_data)\n",
    "print(\"输出维度：\",out_put.shape)#证明最后的输出维度应该不会发生改变\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a0be10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yep\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"yep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dc5c87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
